{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "### Posted: November 2, 2017\n",
    "### Due: November 17, 2017\n",
    "\n",
    "## This project focuses on utilizing the NLP skills we learned to do two basic text analysis tasks. Although the text below references NLTK, you are free to use any other NLP package instead (e.g., spaCy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Review Classification\n",
    "### This part uses the Amazon Fine Foods reviews dataset, that contains reviews for a collection of products from Amazon. You can find the full dataset at: https://snap.stanford.edu/data/web-FineFoods.html. The project directory contains a small portion of it (not a random sample) with 2000 reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task is to read in this file, and construct a simple classifier to predict the review/score from the review/userId, review/profileName, review/time, review/summary, and review/text (but not the productId, or helpfulness). You should figure out different types of features to use for this task, and should use a Naive Bayes Classifier for the classification (you can use other methods if you'd like). You should use off-the-shelf implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I use the code in the following stackoverflow link to read the data and write in csv format.  \n",
    "### https://stackoverflow.com/questions/23331480/converting-amazon-data-into-csv-format-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re \n",
    "\n",
    "INPUT = \"finefoods_training.txt\"\n",
    "OUTPUT = \"Output.csv\"\n",
    "\n",
    "header = [\n",
    "    \"product/productId\",\n",
    "    \"review/userId\",\n",
    "    \"review/profileName\",\n",
    "    \"review/helpfulness\",\n",
    "    \"review/score\",\n",
    "    \"review/time\",\n",
    "    \"review/summary\",\n",
    "    \"review/text\"]\n",
    "\n",
    "f = open(INPUT)\n",
    "outfile = open(OUTPUT,\"w\")\n",
    "\n",
    "# Write header\n",
    "outfile.write(\",\".join(header) + \"\\n\")\n",
    "\n",
    "currentLine = []\n",
    "for line in f:\n",
    "   line = line.strip()\n",
    "\n",
    "   if line == \"\": \n",
    "      outfile.write(\",\".join(currentLine))\n",
    "      outfile.write(\"\\n\")\n",
    "      currentLine = []\n",
    "      continue\n",
    "   parts = line.split(\":\",1)\n",
    "   parts[1] = parts[1].replace(',', ' ')\n",
    "   currentLine.append(parts[1])\n",
    "\n",
    "if currentLine != []:\n",
    "    outfile.write(\",\".join(currentLine))\n",
    "\n",
    "\n",
    "f.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product/productId</th>\n",
       "      <th>review/userId</th>\n",
       "      <th>review/profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1/1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0/0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3/3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product/productId    review/userId                review/profileName  \\\n",
       "0        B001E4KFG0   A3SGXH7AUHU8GW                        delmartian   \n",
       "1        B00813GRG4   A1D87F6ZCVE5NK                            dll pa   \n",
       "2        B000LQOCH0    ABXLMWJIXXAIN   Natalia Corres \"Natalia Corres\"   \n",
       "3        B000UA0QIQ   A395BORC6FGVXV                              Karl   \n",
       "4        B006K2ZZ7K   A1UQRSCLF8GW1T     Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "  review/helpfulness  review/score  review/time          review/summary  \\\n",
       "0                1/1           5.0   1303862400   Good Quality Dog Food   \n",
       "1                0/0           1.0   1346976000       Not as Advertised   \n",
       "2                1/1           4.0   1219017600   \"Delight\" says it all   \n",
       "3                3/3           2.0   1307923200          Cough Medicine   \n",
       "4                0/0           5.0   1350777600             Great taffy   \n",
       "\n",
       "                                         review/text  \n",
       "0   I have bought several of the Vitality canned ...  \n",
       "1   Product arrived labeled as Jumbo Salted Peanu...  \n",
       "2   This is a confection that has been around a f...  \n",
       "3   If you are looking for the secret ingredient ...  \n",
       "4   Great taffy at a great price.  There was a wi...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Output.csv', encoding='latin-1')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I clean review summary and review text columns by lowering all alphabets and also removing non alphabetic parts. Also, the set is devided between 20% test set and 80% train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 items in training data, 400 in test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product/productId</th>\n",
       "      <th>review/userId</th>\n",
       "      <th>review/profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "      <th>Summary_Clean</th>\n",
       "      <th>Text_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1/1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned ...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0/0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanu...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a f...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3/3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient ...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wi...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product/productId    review/userId                review/profileName  \\\n",
       "0        B001E4KFG0   A3SGXH7AUHU8GW                        delmartian   \n",
       "1        B00813GRG4   A1D87F6ZCVE5NK                            dll pa   \n",
       "2        B000LQOCH0    ABXLMWJIXXAIN   Natalia Corres \"Natalia Corres\"   \n",
       "3        B000UA0QIQ   A395BORC6FGVXV                              Karl   \n",
       "4        B006K2ZZ7K   A1UQRSCLF8GW1T     Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "  review/helpfulness  review/score  review/time          review/summary  \\\n",
       "0                1/1           5.0   1303862400   Good Quality Dog Food   \n",
       "1                0/0           1.0   1346976000       Not as Advertised   \n",
       "2                1/1           4.0   1219017600   \"Delight\" says it all   \n",
       "3                3/3           2.0   1307923200          Cough Medicine   \n",
       "4                0/0           5.0   1350777600             Great taffy   \n",
       "\n",
       "                                         review/text          Summary_Clean  \\\n",
       "0   I have bought several of the Vitality canned ...  good quality dog food   \n",
       "1   Product arrived labeled as Jumbo Salted Peanu...      not as advertised   \n",
       "2   This is a confection that has been around a f...    delight says it all   \n",
       "3   If you are looking for the secret ingredient ...         cough medicine   \n",
       "4   Great taffy at a great price.  There was a wi...            great taffy   \n",
       "\n",
       "                                          Text_Clean  \n",
       "0  i have bought several of the vitality canned d...  \n",
       "1  product arrived labeled as jumbo salted peanut...  \n",
       "2  this is a confection that has been around a fe...  \n",
       "3  if you are looking for the secret ingredient i...  \n",
       "4  great taffy at a great price there was a wide ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "cleanup_re = re.compile('[^a-z]+')\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = cleanup_re.sub(' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "df[\"Summary_Clean\"] = df[\"review/summary\"].apply(cleanup)\n",
    "df[\"Text_Clean\"] = df[\"review/text\"].apply(cleanup)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "print(\"%d items in training data, %d in test data\" % (len(train), len(test)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I use CountVectorizer, TfidfTransformer to transform the cleaned summary and cleaned text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(min_df = 1, ngram_range = (1, 4))\n",
    "X_train_summary_counts = count_vect.fit_transform(train[\"Summary_Clean\"])\n",
    "X_test_summary_counts = count_vect.transform(test[\"Summary_Clean\"])\n",
    "\n",
    "X_train_text_counts = count_vect.fit_transform(train[\"Text_Clean\"])\n",
    "X_test_text_counts = count_vect.transform(test[\"Text_Clean\"])\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_summary_tfidf = tfidf_transformer.fit_transform(X_train_summary_counts)\n",
    "X_test_summary_tfidf = tfidf_transformer.transform(X_test_summary_counts)\n",
    "\n",
    "X_train_text_tfidf = tfidf_transformer.fit_transform(X_train_text_counts)\n",
    "X_test_text_tfidf = tfidf_transformer.transform(X_test_text_counts)\n",
    "\n",
    "\n",
    "\n",
    "y_train = train[\"review/score\"]\n",
    "y_test = test[\"review/score\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the most important features that we can train the y on them are transformed cleaned summary and transformed cleaned text. I trained 3 models: Multinomia Naive Bayes, Bernoulli Naive Bayes and Logistic Regression to train y on transformed cleaned text first. Then I will use transformed cleaned summary and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = dict()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train_text_tfidf , y_train)\n",
    "prediction['Multinomial'] = model.predict(X_test_text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X_train_text_tfidf, y_train)\n",
    "prediction['Bernoulli'] = model.predict(X_test_text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg_result = logreg.fit(X_train_text_tfidf, y_train)\n",
    "prediction['Logistic'] = logreg.predict(X_test_text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I look at confusion matrix and accuracy score. They show that the model is not performing very well. The confusion matrix for the first and second model (Naive Bayes) show that all the scores are predicted to be 5 in the test set. The logistic regression is performing a little bit better but it also mispredict lots of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,   0,  23],\n",
       "       [  0,   0,   0,   0,  39],\n",
       "       [  0,   0,   0,   0,  54],\n",
       "       [  0,   0,   0,   0, 255]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(y_test,prediction['Multinomial'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,   0,  23],\n",
       "       [  0,   0,   0,   0,  39],\n",
       "       [  0,   0,   0,   0,  54],\n",
       "       [  0,   0,   0,   0, 255]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction['Bernoulli'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8,   0,   1,   1,  19],\n",
       "       [  2,   0,   2,   2,  17],\n",
       "       [  4,   1,   1,   4,  29],\n",
       "       [  2,   0,   1,   3,  48],\n",
       "       [  1,   0,   0,   4, 250]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction['Logistic'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63749999999999996"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,prediction['Multinomial'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63749999999999996"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,prediction['Bernoulli'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65500000000000003"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,prediction['Logistic'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I use transformed cleaned summary and try to predict y. The results are still very bad. Even worse than the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = dict()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train_summary_tfidf , y_train)\n",
    "prediction2['Multinomial'] = model.predict(X_test_summary_tfidf)\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X_train_summary_tfidf, y_train)\n",
    "prediction2['Bernoulli'] = model.predict(X_test_summary_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg_result = logreg.fit(X_train_summary_tfidf, y_train)\n",
    "prediction2['Logistic'] = logreg.predict(X_test_summary_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   0,   0,   0,  28],\n",
       "       [  0,   0,   0,   0,  23],\n",
       "       [  0,   0,   0,   0,  39],\n",
       "       [  0,   0,   0,   0,  54],\n",
       "       [  0,   0,   0,   0, 255]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction2['Multinomial'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,   0,  23],\n",
       "       [  0,   0,   0,   0,  39],\n",
       "       [  0,   0,   0,   0,  54],\n",
       "       [  0,   0,   0,   0, 255]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction2['Bernoulli'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16,   2,   2,   1,   8],\n",
       "       [  7,   2,   2,   0,  12],\n",
       "       [  4,   3,   6,   1,  25],\n",
       "       [  1,   1,   3,   5,  44],\n",
       "       [  7,   3,   4,  15, 226]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction2['Logistic'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63749999999999996"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,prediction2['Logistic'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Named Entity Recognition\n",
    "\n",
    "## Here, we will use the Named Entity Recognition functionality of NLTK to extract entities and relationships from some recent news articles. Specifically, pick your favorite viewer sport (football, soccer, tennis, baseball, cricket, etc.), and download 10 recent news articles describing some games/matches.\n",
    "## Use NLTK to write code to extract named entities from each of them. The final output should simply be a list of entities and their types, which would require understanding the structure of the output of the ne_chunk command, and traversing it to find just the named entities.\n",
    "## Next, write a few regular expressions to extract information about which positions or roles different players serve in their team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I defined a function that get a text, word_tokenize it first and do pos_tagging and then find the name entities. The output is a list of name entities and their types. I have shown the type of the output by a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entity(document):\n",
    "    output=[]\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            output+=[[' '.join(c[0] for c in chunk),chunk.label()]]\n",
    "            #print(' '.join(c[0] for c in chunk),\":\", chunk.label())\n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WASHINGTON', 'GPE'],\n",
       " ['New York', 'GPE'],\n",
       " ['Loretta E. Lynch', 'PERSON'],\n",
       " ['Brooklyn', 'GPE']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_named_entity(\"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I defined a list of urls that are 10 different news articles related to soccer. I used requests and BeautifulSoup to just read the title and body of the news from each one of these articles and at the end made a list of all the name entities and their type for each one of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Italy', 'GPE'],\n",
       "  ['Stuns Soccer Fans', 'PERSON'],\n",
       "  ['Fails', 'PERSON'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Sweden', 'PERSON'],\n",
       "  ['David Greene', 'PERSON'],\n",
       "  ['Christopher Livesay', 'PERSON'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Sweden', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Gigi Buffon', 'PERSON'],\n",
       "  ['SOUNDBITE OF', 'ORGANIZATION'],\n",
       "  ['ARCHIVED', 'ORGANIZATION'],\n",
       "  ['GIGI', 'ORGANIZATION'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Journalist Christopher Livesay', 'PERSON'],\n",
       "  ['Rome', 'GPE'],\n",
       "  ['Chris', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'PERSON'],\n",
       "  ['Yankees', 'ORGANIZATION'],\n",
       "  ['GREENE', 'GPE'],\n",
       "  ['Yankees', 'ORGANIZATION'],\n",
       "  ['Yankees', 'ORGANIZATION'],\n",
       "  ['Buffon', 'PERSON'],\n",
       "  ['Italians', 'GPE'],\n",
       "  ['Gian Piero Ventura', 'PERSON'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Serie As', 'PERSON'],\n",
       "  ['non-Italian', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Chris Livesay', 'PERSON'],\n",
       "  ['Rome', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Chris', 'PERSON'],\n",
       "  ['SOUNDBITE OF', 'ORGANIZATION'],\n",
       "  ['CON', 'ORGANIZATION'],\n",
       "  ['PARTIRO', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['Italy', 'GPE'],\n",
       "  ['Misses', 'PERSON'],\n",
       "  ['First Time', 'PERSON'],\n",
       "  ['NPRELISE', 'ORGANIZATION'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['Italy', 'PERSON'],\n",
       "  ['Sweden', 'GPE'],\n",
       "  ['La Gazzetta', 'PERSON'],\n",
       "  ['Sport', 'PERSON'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Gianluigi Buffon', 'PERSON'],\n",
       "  ['SOUNDBITE OF', 'ORGANIZATION'],\n",
       "  ['ARCHIVED', 'ORGANIZATION'],\n",
       "  ['GIANLUIGI', 'ORGANIZATION'],\n",
       "  ['Italian', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Paolo Bandini', 'PERSON'],\n",
       "  ['European', 'GPE'],\n",
       "  ['Skype', 'GPE'],\n",
       "  ['Paolo', 'PERSON'],\n",
       "  ['Italy', 'PERSON'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Sweden', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'PERSON'],\n",
       "  ['Gian Piero Ventura', 'PERSON'],\n",
       "  ['Champions League', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Italians', 'GPE'],\n",
       "  ['Italians', 'GPE'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Gazzetta', 'ORGANIZATION'],\n",
       "  ['Italy', 'GPE'],\n",
       "  ['Paolo Bandini', 'PERSON'],\n",
       "  ['Guardian', 'GPE'],\n",
       "  ['ESPN', 'ORGANIZATION'],\n",
       "  ['Paolo', 'PERSON'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['U.S.', 'GPE'],\n",
       "  ['Team Defeats Jamaica For Title', 'ORGANIZATION'],\n",
       "  ['NPRJordan Morris', 'ORGANIZATION'],\n",
       "  ['Team', 'ORGANIZATION'],\n",
       "  ['Jamaica', 'PERSON'],\n",
       "  ['CONCACAF', 'ORGANIZATION'],\n",
       "  ['Levi', 'ORGANIZATION'],\n",
       "  ['Santa Clara', 'GPE'],\n",
       "  ['Jordan Morris', 'PERSON'],\n",
       "  ['Jamaica', 'GPE'],\n",
       "  ['Gold', 'ORGANIZATION'],\n",
       "  ['Watson', 'PERSON'],\n",
       "  ['Jamaica', 'PERSON'],\n",
       "  ['Jamaican', 'GPE'],\n",
       "  ['Dwayne Miller', 'PERSON'],\n",
       "  ['Americans', 'GPE'],\n",
       "  ['Morris', 'PERSON'],\n",
       "  ['Jozy Altidore', 'PERSON'],\n",
       "  ['Bruce Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Graham Zusi', 'PERSON'],\n",
       "  ['Arlington', 'GPE'],\n",
       "  ['Texas', 'GPE'],\n",
       "  ['Jamaican', 'GPE'],\n",
       "  ['Whitmore', 'PERSON'],\n",
       "  ['Michael Bradley', 'PERSON'],\n",
       "  ['Jamaica', 'PERSON'],\n",
       "  ['Andre Blake', 'PERSON'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Clint Dempsey', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Costa Rica', 'PERSON'],\n",
       "  ['Gold', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['USMNT', 'ORGANIZATION'],\n",
       "  ['Comes Next', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Trinidad', 'GPE'],\n",
       "  ['Tobago', 'GPE'],\n",
       "  ['ROBERT', 'ORGANIZATION'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['SOUNDBITE OF', 'ORGANIZATION'],\n",
       "  ['ARCHIVED', 'ORGANIZATION'],\n",
       "  ['UNIDENTIFIED', 'ORGANIZATION'],\n",
       "  ['Long', 'GPE'],\n",
       "  ['Alvin Jones', 'PERSON'],\n",
       "  ['Trinidadian', 'GPE'],\n",
       "  ['Trinidad', 'GPE'],\n",
       "  ['Tobago', 'PERSON'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Roger Bennett', 'PERSON'],\n",
       "  ['Blazers', 'ORGANIZATION'],\n",
       "  ['NBC Sports', 'ORGANIZATION'],\n",
       "  ['Robert', 'PERSON'],\n",
       "  ['American', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Armageddon', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['America', 'GPE'],\n",
       "  ['Tim Howard', 'PERSON'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Tim Howard', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Belgium', 'GPE'],\n",
       "  ['Bruce Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Robert', 'PERSON'],\n",
       "  ['Chile', 'PERSON'],\n",
       "  ['Netherlands', 'PERSON'],\n",
       "  ['America', 'GPE'],\n",
       "  ['Roger Bennett', 'PERSON'],\n",
       "  ['Blazers', 'GPE'],\n",
       "  ['NBC Sports', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['U.S.', 'GPE'],\n",
       "  ['Men', 'ORGANIZATION'],\n",
       "  ['Soccer Goes', 'ORGANIZATION'],\n",
       "  ['Future With New Coach', 'ORGANIZATION'],\n",
       "  ['New Priorities', 'ORGANIZATION'],\n",
       "  ['Bruce Arena', 'PERSON'],\n",
       "  ['Carson', 'GPE'],\n",
       "  ['Calif', 'GPE'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Serbia', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Jurgen Klinsmann', 'PERSON'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Russia', 'GPE'],\n",
       "  ['January CampThe', 'FACILITY'],\n",
       "  ['Carson', 'GPE'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['Top U.S.', 'PERSON'],\n",
       "  ['California', 'GPE'],\n",
       "  ['Carson', 'PERSON'],\n",
       "  ['StubHub Center', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['Costa Rica', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Carson', 'GPE'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Darlington Nagbe', 'PERSON'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['Portland', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Carson', 'GPE'],\n",
       "  ['Calif', 'GPE'],\n",
       "  ['USMNT', 'ORGANIZATION'],\n",
       "  ['Graham Zusi', 'PERSON'],\n",
       "  ['prioritiesThe', 'ORGANIZATION'],\n",
       "  ['Honduras', 'GPE'],\n",
       "  ['Panama', 'GPE'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['Costa Rica', 'PERSON'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Bruce', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Michael Bradley', 'PERSON'],\n",
       "  ['Midfielder Graham Zusi', 'ORGANIZATION'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Zusi', 'PERSON'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Klinsmann', 'PERSON'],\n",
       "  ['Zusi', 'PERSON'],\n",
       "  ['playersIn', 'ORGANIZATION'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Klinsmann', 'PERSON'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['Klinsmann', 'PERSON'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['L.A. Galaxy', 'ORGANIZATION'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['MLS', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Klinsmann', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['Russia', 'GPE'],\n",
       "  ['Bruce Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Honduras', 'GPE'],\n",
       "  ['Panama', 'ORGANIZATION'],\n",
       "  ['Arena', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['Abby Wambach', 'PERSON'],\n",
       "  ['NPRAbby Wambach', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Japan', 'GPE'],\n",
       "  ['FIFA Women', 'ORGANIZATION'],\n",
       "  ['Wambach', 'PERSON'],\n",
       "  ['Sarah Huffman', 'PERSON'],\n",
       "  ['Sarah Huffman', 'PERSON'],\n",
       "  ['Sarah', 'PERSON'],\n",
       "  ['Sarah', 'PERSON'],\n",
       "  ['Catholic', 'ORGANIZATION'],\n",
       "  ['herGod', 'ORGANIZATION'],\n",
       "  ['DUI', 'ORGANIZATION'],\n",
       "  ['arrestI', 'ORGANIZATION'],\n",
       "  ['DUI', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['Brazil', 'PERSON'],\n",
       "  ['Men', 'ORGANIZATION'],\n",
       "  ['Soccer Redeem Loss To Germany For Olympic Gold', 'ORGANIZATION'],\n",
       "  ['Penalty', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Brazil', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['MARTIN', 'ORGANIZATION'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Brazilian', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['NPR', 'ORGANIZATION'],\n",
       "  ['Melissa Block', 'PERSON'],\n",
       "  ['BLOCK', 'ORGANIZATION'],\n",
       "  ['BYLINE', 'GPE'],\n",
       "  ['Maracana Stadium', 'PERSON'],\n",
       "  ['Brazilians', 'ORGANIZATION'],\n",
       "  ['Portuguese', 'GPE'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Brazil', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Maracana', 'PERSON'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Neymar', 'GPE'],\n",
       "  ['German', 'GPE'],\n",
       "  ['Maracana', 'PERSON'],\n",
       "  ['Portuguese', 'GPE'],\n",
       "  ['Germany', 'PERSON'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Neymar', 'GPE'],\n",
       "  ['Delirium', 'PERSON'],\n",
       "  ['Brazilian', 'GPE'],\n",
       "  ['Germans', 'GPE'],\n",
       "  ['Luciana Gomes', 'PERSON'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Fan Adriano Sancho', 'PERSON'],\n",
       "  ['ADRIANO', 'ORGANIZATION'],\n",
       "  ['Portuguese', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Portuguese', 'GPE'],\n",
       "  ['Douglas Florencio', 'PERSON'],\n",
       "  ['DOUGLAS', 'ORGANIZATION'],\n",
       "  ['Brazil', 'PERSON'],\n",
       "  ['Brazilian', 'GPE'],\n",
       "  ['Melissa Block', 'PERSON'],\n",
       "  ['NPR News', 'ORGANIZATION'],\n",
       "  ['Rio', 'PERSON'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['U.S.', 'GPE'],\n",
       "  ['Soccer Team Faces Argentina', 'PERSON'],\n",
       "  ['Copa', 'GPE'],\n",
       "  ['NPRAUDIE CORNISH', 'ORGANIZATION'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['Tonight', 'PERSON'],\n",
       "  ['Houston', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Copa America', 'ORGANIZATION'],\n",
       "  ['FIFA', 'ORGANIZATION'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Lionel Messi', 'PERSON'],\n",
       "  ['Houston Chronicle', 'PERSON'],\n",
       "  ['Martin Hajovsky', 'PERSON'],\n",
       "  ['U.S.MARTIN', 'ORGANIZATION'],\n",
       "  ['Houston', 'GPE'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Confederations Cup', 'ORGANIZATION'],\n",
       "  ['South Africa', 'GPE'],\n",
       "  ['Brazil', 'GPE'],\n",
       "  ['Spain', 'GPE'],\n",
       "  ['Copa America', 'PERSON'],\n",
       "  ['South America', 'GPE'],\n",
       "  ['South America', 'GPE'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['Got', 'PERSON'],\n",
       "  ['American', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Germany', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Copa America', 'ORGANIZATION'],\n",
       "  ['Lionel Messi', 'PERSON'],\n",
       "  ['Messi', 'PERSON'],\n",
       "  ['Champions Leagues', 'PERSON'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Pele', 'PERSON'],\n",
       "  ['Maradona', 'GPE'],\n",
       "  ['World Cups', 'PERSON'],\n",
       "  ['Copa Americas', 'PERSON'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Houston', 'GPE'],\n",
       "  ['Houston', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Census Bureau', 'ORGANIZATION'],\n",
       "  ['America', 'GPE'],\n",
       "  ['Colombia', 'GPE'],\n",
       "  ['Costa Rica', 'PERSON'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['Houston', 'GPE'],\n",
       "  ['Colombians', 'GPE'],\n",
       "  ['Chileans', 'GPE'],\n",
       "  ['Costa Ricans', 'ORGANIZATION'],\n",
       "  ['Argentinian', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['Jurgen Klinsmann', 'PERSON'],\n",
       "  ['Martin Hajovsky', 'PERSON'],\n",
       "  ['Houston Chronicle', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Eastern', 'LOCATION'],\n",
       "  ['Martin', 'PERSON'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['Iceland', 'GPE'],\n",
       "  ['Eliminates Austria', 'ORGANIZATION'],\n",
       "  ['European', 'GPE'],\n",
       "  ['Soccer', 'ORGANIZATION'],\n",
       "  ['NPRFor Iceland', 'ORGANIZATION'],\n",
       "  ['European', 'ORGANIZATION'],\n",
       "  ['GREENE', 'ORGANIZATION'],\n",
       "  ['HOST', 'ORGANIZATION'],\n",
       "  ['David Greene', 'PERSON'],\n",
       "  ['Iceland', 'GPE'],\n",
       "  ['Euro', 'GPE'],\n",
       "  ['Iceland', 'GPE'],\n",
       "  ['Austria', 'GPE'],\n",
       "  ['Iceland', 'GPE'],\n",
       "  ['SOUNDBITE OF', 'ORGANIZATION'],\n",
       "  ['ARCHIVED', 'ORGANIZATION'],\n",
       "  ['UNIDENTIFIED', 'ORGANIZATION'],\n",
       "  ['England', 'GPE'],\n",
       "  ['MORNING', 'ORGANIZATION'],\n",
       "  ['NPR', 'ORGANIZATION']],\n",
       " [['U.S.', 'GPE'],\n",
       "  ['Makes Copa America Semifinals', 'PERSON'],\n",
       "  ['Win Over Ecuador', 'PERSON'],\n",
       "  ['Associated', 'ORGANIZATION'],\n",
       "  ['Clint Dempsey', 'PERSON'],\n",
       "  ['Ecuador', 'PERSON'],\n",
       "  ['Seattle', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Dempsey', 'PERSON'],\n",
       "  ['Gyasi Zardes', 'PERSON'],\n",
       "  ['United States', 'GPE'],\n",
       "  ['Copa America', 'ORGANIZATION'],\n",
       "  ['Ecuador', 'PERSON'],\n",
       "  ['Pacific Northwest', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Argentina', 'GPE'],\n",
       "  ['Venezuela', 'PERSON'],\n",
       "  ['Houston', 'GPE'],\n",
       "  ['Fox Sports', 'PERSON'],\n",
       "  ['Landon Donovan', 'PERSON'],\n",
       "  ['American', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Zardes', 'PERSON'],\n",
       "  ['Dempsey', 'PERSON'],\n",
       "  ['Arroyo', 'PERSON'],\n",
       "  ['Ecuador', 'ORGANIZATION'],\n",
       "  ['Enner Valencia', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['American', 'GPE'],\n",
       "  ['Jermaine Jones', 'PERSON'],\n",
       "  ['Ecuador', 'PERSON'],\n",
       "  ['Antonio Valencia', 'PERSON'],\n",
       "  ['Alejando Bedoya', 'PERSON'],\n",
       "  ['Valencia', 'PERSON'],\n",
       "  ['Jones', 'PERSON'],\n",
       "  ['Arroyo', 'ORGANIZATION'],\n",
       "  ['Colombian', 'GPE'],\n",
       "  ['Wilmar Roldan', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Alejandro Bedoya', 'PERSON'],\n",
       "  ['Bobby Wood', 'PERSON'],\n",
       "  ['Gustavo Quinteros', 'PERSON'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Spain', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Copa', 'ORGANIZATION'],\n",
       "  ['Mexico', 'GPE'],\n",
       "  ['Brazil.Dempsey', 'PERSON'],\n",
       "  ['Seattle Sounders', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Wood', 'PERSON'],\n",
       "  ['American', 'GPE'],\n",
       "  ['Dempsey', 'PERSON'],\n",
       "  ['Wood', 'GPE'],\n",
       "  ['Jones', 'PERSON'],\n",
       "  ['Dempsey', 'PERSON'],\n",
       "  ['Juan Paredes', 'PERSON'],\n",
       "  ['Alexander Dominguez', 'PERSON'],\n",
       "  ['Wood', 'GPE'],\n",
       "  ['Brooks', 'PERSON'],\n",
       "  ['Wood', 'GPE'],\n",
       "  ['Matt Besler', 'PERSON'],\n",
       "  ['Gyasi Zardes', 'PERSON'],\n",
       "  ['Dempsey', 'PERSON'],\n",
       "  ['Zardes', 'GPE'],\n",
       "  ['Ecuador', 'PERSON'],\n",
       "  ['Arroyo', 'PERSON'],\n",
       "  ['Walter Ayovi', 'PERSON'],\n",
       "  ['Enner Valencia', 'PERSON'],\n",
       "  ['Brooks', 'ORGANIZATION'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['DeAndre Yedlin', 'ORGANIZATION'],\n",
       "  ['Paraguay', 'GPE'],\n",
       "  ['U.S.', 'GPE'],\n",
       "  ['Jurgen Klinsmann', 'PERSON'],\n",
       "  ['Besler', 'PERSON'],\n",
       "  ['Fabian Johnson', 'PERSON'],\n",
       "  ['Yedlin', 'PERSON'],\n",
       "  ['Michael Bradley', 'PERSON'],\n",
       "  ['Orlando', 'GPE'],\n",
       "  ['NPR', 'ORGANIZATION']]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "list_of_urls = [\"https://www.npr.org/2017/11/14/564006451/italy-stuns-soccer-fans-fails-to-qualify-for-world-cup\",\n",
    "                \"https://www.npr.org/2017/11/14/564163477/italy-misses-world-cup-qualifier-for-first-time-since-1958\",\n",
    "                \"https://www.npr.org/2017/07/26/539450573/a-comeback-could-be-on-the-horizon-for-u-s-men-s-soccer\",\n",
    "                \"https://www.npr.org/2017/10/11/557198136/after-devastating-loss-for-usmnt-what-comes-next\",\n",
    "                \"https://www.npr.org/2017/01/25/511608910/u-s-mens-soccer-goes-back-to-the-future-with-new-coach-new-priorities\",\n",
    "                \"https://www.npr.org/2016/09/14/493831738/why-abby-wambach-doesnt-want-to-be-known-just-as-a-soccer-player\",\n",
    "                \"https://www.npr.org/2016/08/21/490819550/brazil-men-s-soccer-redeem-loss-to-germany-for-olympic-gold-in-penalty-kick-shoo\",\n",
    "                \"https://www.npr.org/2016/06/21/482981926/u-s-soccer-team-faces-argentina-in-copa-semi-final\",\n",
    "                \"https://www.npr.org/2016/06/23/483197692/iceland-eliminates-austria-in-european-soccer-tournament\",\n",
    "                \"https://www.npr.org/2016/06/17/482420749/u-s-mens-soccer-tops-ecuador-to-make-copa-america-semifinals\"]\n",
    "list_of_named_entity=[]\n",
    "for url_soccer in list_of_urls:\n",
    "    r = requests.get(url_soccer)\n",
    "    text = r.text\n",
    "\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    bodytext=soup.findAll([\"p\",\"title\"])\n",
    "\n",
    "    list_of_data=[]\n",
    "    for p in bodytext:\n",
    "        if p.string !=None:\n",
    "            list_of_data.append(p.string)\n",
    "        \n",
    "    document=\"\"\n",
    "    for i in list_of_data:\n",
    "        document+=i\n",
    "    \n",
    "    \n",
    "    list_of_named_entity.append(get_named_entity(document))\n",
    "\n",
    "list_of_named_entity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sent2= \"ali is a goalkeeper and davide is striker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sem import extract_rels,rtuple\n",
    "IN = re.compile(r'.*\\bgoalkeeper\\b.|\\bdefender\\b.|\\bmidfielder\\b.|\\bforward\\b.|\\bstriker\\b.*')\n",
    "rels = extract_rels('PER', 'PER', my_sent2, pattern=IN) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
